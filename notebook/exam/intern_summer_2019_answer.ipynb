{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 株式会社ALBERT 2019年度夏季インターン選抜課題\n",
    "## 回答条件・提出物について\n",
    "### 条件\n",
    "- 回答には python 3.6 以上を用いること．\n",
    "- 使用ライブラリに制限は設けない．\n",
    "- 回答期限は１週間です．全問解けている必要はありません．可能な範囲で回答し提出してください．\n",
    "\n",
    "### 提出物\n",
    "- 回答を記述した本ノートブック．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題\n",
    "強化学習とは、 **エージェント (agent)** の行動決定問題を扱う機械学習分野の一領域である。  \n",
    "エージェントは周囲の環境の **状態 (state)** を観測し **行動 (action)** を選択する。  \n",
    "その際、環境から得られる **報酬 (reward)** を最大化するように行動指針を更新することで、適切な行動戦略を学習する。\n",
    "\n",
    "本課題では強化学習の一種である **Q 学習** を扱う。  \n",
    "Q 学習ではエージェントに **Q 値** という値を持たせ、これを各行動の有効性を示したものと考える。  \n",
    "エージェントは環境の中で行動を繰り返し、得られた報酬をもとに Q 値を調整してゆくことで、だんだんと賢くなっていくのである。\n",
    "\n",
    "さて、以下にあるのは **○×ゲーム (Tic-tac-toe)** のプログラムコードである。○×ゲームのルールは次の通りである。\n",
    "1. 3x3 のマス目を用意する\n",
    "1. 先手・後手がそれぞれ交互に○と×をマス目に書き込んでいく\n",
    "1. 縦横斜めいずれか一列を先に自分のマークで埋めた方が勝ち\n",
    "\n",
    "今回の実装では、これを次のように実現している。\n",
    "1. TicTacToe クラスは、Player クラスを内部に持つ\n",
    "1. TicTacToe クラスは、先手後手交互に Player クラスの action メソッドを呼ぶ\n",
    "1. action メソッドの戻り値をうけて TicTacToe クラスはゲームを進め、勝敗が決すれば Player クラスの finalize メソッドを呼ぶ\n",
    "\n",
    "TicTacToe クラスは action に際し、盤面情報 (board) を Player に渡すが、その盤面情報は渡す相手に合わせて normalize されていることに注意されたい。  \n",
    "すなわち、渡す相手のマークを MY_MARK, 渡す相手の敵のマークを ENEMY_MARK に変換してから board 配列を渡している。\n",
    "\n",
    "本課題の目的は、この○×ゲームを Q 学習によって攻略することである。  \n",
    "問題文に従ってプログラムを実装し、○×ゲームをプレイするエージェントを実装しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN = 1\n",
    "DRAW = 0\n",
    "LOSE = -1\n",
    "\n",
    "EMPTY = 0\n",
    "MY_MARK = 1\n",
    "ENEMY_MARK = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ゲーム盤クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe(object):\n",
    "\n",
    "    NUM_EMPTY = 0\n",
    "    NUM_X = 1\n",
    "    NUM_O = -1\n",
    "\n",
    "    def __init__(self, X, O):\n",
    "        self.X = X\n",
    "        self.O = O\n",
    "        self.board = np.full(9, self.NUM_EMPTY, dtype=np.int32)\n",
    "        self.num2player = {self.NUM_X: self.X,\n",
    "                           self.NUM_O: self.O}\n",
    "        self.num2mark = {self.NUM_EMPTY: ' ',\n",
    "                         self.NUM_X: 'X',\n",
    "                         self.NUM_O: 'O'}\n",
    "\n",
    "    def opposite_turn(self, turn):\n",
    "        if turn == self.NUM_X:\n",
    "            return self.NUM_O\n",
    "        else:\n",
    "            return self.NUM_X\n",
    "\n",
    "    def normalized_board(self, turn):\n",
    "        board = np.full(9, EMPTY, dtype=np.int32)\n",
    "        board[self.board == turn] = MY_MARK\n",
    "        board[self.board == self.opposite_turn(turn)] = ENEMY_MARK\n",
    "        return board\n",
    "\n",
    "    def move_and_judge(self, pos, turn):\n",
    "        winner = None\n",
    "        loser = None\n",
    "        game_is_over = False\n",
    "\n",
    "        winning_conditions = tuple([\n",
    "            [[0, 1, 2], [3, 4, 5], [6, 7, 8],\n",
    "             [0, 3, 6], [1, 4, 7], [2, 5, 8],\n",
    "             [0, 4, 8], [2, 4, 6]]\n",
    "        ])\n",
    "\n",
    "        if self.board[pos] != self.NUM_EMPTY:\n",
    "            game_is_over = True\n",
    "            winner = self.opposite_turn(turn)\n",
    "            loser = turn\n",
    "            return game_is_over, winner, loser\n",
    "\n",
    "        # update board.\n",
    "        self.board[pos] = turn\n",
    "\n",
    "        if not (self.board == self.NUM_EMPTY).any():\n",
    "            game_is_over = True\n",
    "\n",
    "        if (self.board[winning_conditions] == turn).all(axis=1).any():\n",
    "            winner = turn\n",
    "            loser = self.opposite_turn(turn)\n",
    "            game_is_over = True\n",
    "\n",
    "        return game_is_over, winner, loser\n",
    "\n",
    "    def print_board(self):\n",
    "        marks = [self.num2mark[num] for num in self.board]\n",
    "        board_text = (\n",
    "            ' {} | {} | {} \\n-----------\\n'\n",
    "            ' {} | {} | {} \\n-----------\\n'\n",
    "            ' {} | {} | {}'.format(*marks))\n",
    "        print(board_text + '\\n')\n",
    "\n",
    "    def print_actions(self):\n",
    "        board_text = (\n",
    "            ' {} | {} | {} \\n-----------\\n'\n",
    "            ' {} | {} | {} \\n-----------\\n'\n",
    "            ' {} | {} | {}'.format(*np.arange(9)))\n",
    "        print('actions: \\n' + board_text + '\\n')\n",
    "\n",
    "    def main_loop(self, print_game=False):\n",
    "        turn = self.NUM_X\n",
    "        game_is_over = False\n",
    "\n",
    "        if print_game:\n",
    "            self.print_actions()\n",
    "\n",
    "        while not game_is_over:\n",
    "            if print_game:\n",
    "                print('{}({})\\'s turn.'.format(\n",
    "                    self.num2player[turn].name, self.num2mark[turn]))\n",
    "                self.print_board()\n",
    "\n",
    "            player = self.num2player[turn]\n",
    "\n",
    "            normalized_board = self.normalized_board(turn)\n",
    "            pos = player.action(normalized_board)\n",
    "            game_is_over, winner, loser = self.move_and_judge(pos, turn)\n",
    "\n",
    "            turn = self.opposite_turn(turn)\n",
    "\n",
    "        if print_game:\n",
    "            self.print_board()\n",
    "            print('GAME is OVER!')\n",
    "            if winner is None:\n",
    "                print('DRAW')\n",
    "            else:\n",
    "                print('WINNER: {}'.format(self.num2player[winner].name))\n",
    "\n",
    "        if winner is None:  # DRAW\n",
    "            self.X.finalize(DRAW, self.normalized_board(self.NUM_X))\n",
    "            self.O.finalize(DRAW, self.normalized_board(self.NUM_O))\n",
    "        else:\n",
    "            self.num2player[winner].finalize(\n",
    "                WIN, self.normalized_board(winner))\n",
    "            self.num2player[loser].finalize(\n",
    "                LOSE, self.normalized_board(loser))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player（人間）クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer(object):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def action(self, board):\n",
    "        a = int(input())\n",
    "        return a\n",
    "\n",
    "    def finalize(self, condition, board):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: \n",
      " 0 | 1 | 2 \n",
      "-----------\n",
      " 3 | 4 | 5 \n",
      "-----------\n",
      " 6 | 7 | 8\n",
      "\n",
      "YAMAUCHI(X)'s turn.\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |  \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KATAGIRI(O)'s turn.\n",
      " X |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |  \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |  \n",
      "\n",
      "GAME is OVER!\n",
      "WINNER: YAMAUCHI\n"
     ]
    }
   ],
   "source": [
    "# 実行例\n",
    "human1 = HumanPlayer('YAMAUCHI')\n",
    "human2 = HumanPlayer('KATAGIRI')\n",
    "\n",
    "game = TicTacToe(human1, human2)\n",
    "game.main_loop(print_game=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問１\n",
    "board を受け取って、可能なマス目をランダムに選んで行動する RandomPlayer クラスをつくりたい。  \n",
    "以下の RandomPlayer クラスの action メソッドを完成させ、RandomPlayer 同士の対局を実行しなさい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player（ランダム）クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer(object):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def action(self, board):\n",
    "        a_candidate = np.arange(9)[board == EMPTY]\n",
    "        return np.random.choice(a_candidate)\n",
    "\n",
    "    def finalize(self, condition, board):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: \n",
      " 0 | 1 | 2 \n",
      "-----------\n",
      " 3 | 4 | 5 \n",
      "-----------\n",
      " 6 | 7 | 8\n",
      "\n",
      "乱太郎(X)'s turn.\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |  \n",
      "\n",
      "乱次郎(O)'s turn.\n",
      "   |   | X \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |  \n",
      "\n",
      "乱太郎(X)'s turn.\n",
      "   |   | X \n",
      "-----------\n",
      " O |   |   \n",
      "-----------\n",
      "   |   |  \n",
      "\n",
      "乱次郎(O)'s turn.\n",
      "   |   | X \n",
      "-----------\n",
      " O |   |   \n",
      "-----------\n",
      " X |   |  \n",
      "\n",
      "乱太郎(X)'s turn.\n",
      "   |   | X \n",
      "-----------\n",
      " O |   |   \n",
      "-----------\n",
      " X | O |  \n",
      "\n",
      "乱次郎(O)'s turn.\n",
      " X |   | X \n",
      "-----------\n",
      " O |   |   \n",
      "-----------\n",
      " X | O |  \n",
      "\n",
      "乱太郎(X)'s turn.\n",
      " X |   | X \n",
      "-----------\n",
      " O | O |   \n",
      "-----------\n",
      " X | O |  \n",
      "\n",
      "乱次郎(O)'s turn.\n",
      " X |   | X \n",
      "-----------\n",
      " O | O | X \n",
      "-----------\n",
      " X | O |  \n",
      "\n",
      "乱太郎(X)'s turn.\n",
      " X |   | X \n",
      "-----------\n",
      " O | O | X \n",
      "-----------\n",
      " X | O | O\n",
      "\n",
      " X | X | X \n",
      "-----------\n",
      " O | O | X \n",
      "-----------\n",
      " X | O | O\n",
      "\n",
      "GAME is OVER!\n",
      "WINNER: 乱太郎\n"
     ]
    }
   ],
   "source": [
    "random1 = RandomPlayer('乱太郎')\n",
    "random2 = RandomPlayer('乱次郎')\n",
    "\n",
    "game = TicTacToe(random1, random2)\n",
    "game.main_loop(print_game=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 学習では、Q 値を用いて行動決定を行う。  \n",
    "Q 値はすべての (状態 $s$, 行動 $a$) の組に対して定義され、状態 $s$ において行動 $a$ を選択する有効性を示す。  \n",
    "ここで、状態 $s$ における 行動 $a$ の Q 値を $ Q(s, a) $ と書くことにする。\n",
    "\n",
    "Q 学習ではエージェントは状態 $s$ を観測し、Q 値を最大化する行動 $a$ を選択する。すなわち、状態 $s$ における行動 $ a(s) $ は、\n",
    "$$\n",
    "a(s) = argmax_a Q(s, a)\n",
    "$$\n",
    "で表される。  \n",
    "\n",
    "Q 値の更新は次のように行われる。\n",
    "$$\n",
    "Q^{new}(s, a) = Q(s, a) + lr \\cdot (r + \\gamma \\cdot max_a Q(s', a) - Q(s, a))\n",
    "$$\n",
    "ただし、 $lr$ は **学習率**、$\\gamma$ は **割引率**、 $r$ は 状態 $s$ において行動 $a$ を取った直後に得られた **報酬**、 $s'$ は 状態 $s$ において行動 $a$ を取った直後に観測された状況である。  \n",
    "\n",
    "更新式から明らかなように、状態 $s$ で行動 $a$ を取った際に正の $r$ が得られたとすると $Q(s, a)$ の値は大きくなる。  \n",
    "したがって、良い行動を取ったときに大きな $r$ を与え、間違ったときに小さな $r$ を与えるようしてやれば、エージェントの行動は改善されていくと考えられる。  \n",
    "どのような報酬 $r$ を与えるか（報酬設計）は強化学習一般において重要な問題である。\n",
    "\n",
    "さて、○×ゲームのような環境では、勝敗が決するまで、途中の各行動が良かったのかどうか判定できない。  \n",
    "そのため、勝敗に通じる最後の行動以外については適切な報酬を与えることが難しい。  \n",
    "Q 学習では状態 $s'$ における Q 値の（$a$ をいろいろ動かしたときの）最大値を更新時に加えることで、この問題に対処している。  \n",
    "次の時点での Q 値（の最大値）を現在の Q 値に伝播させることで、最後にのみ与えられる報酬の影響を、それ以前に遡って分配することができるのである。  \n",
    "（割引率 $\\gamma$ は、未来の報酬にどの程度影響を受けるかを制御するハイパーパラメタである。）\n",
    "\n",
    "ところで、上記のように Q 値を更新すると、エージェントは特定の行動ばかりを取るようになってしまうことが考えられる。  \n",
    "そうなるとエージェントは新しい局面に触れることがなくなり、学習が進まなくなってしまう場合がある。  \n",
    "それを防ぐ方法として、確率 $\\epsilon$ でランダムな行動を取る ε-greedy 法などのアルゴリズムが知られている。  \n",
    "ランダムに環境を探索させることで、エージェントにより詳しく環境について学んでもらおうという企みである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の実装では、Q 値を QTable クラスとして実装している。  \n",
    "QTable クラスは、状態 $s$ を Key，各行動 $a$ に対する Q 値（の配列）を Value とする辞書を内部に持ち、getitem メソッド、setitem メソッドによって Q 値を取り出したり書き換えたりできるようになっている。\n",
    "\n",
    "また、Q 値を更新するには、$s$, $a$, $s'$, $r$ を保存しておく必要があるので、これらを記憶する Memory クラスを用意した。  \n",
    "Memory クラスは $s$, $a$, $s'$, $r$ 及び、エピソード終了を表すフラグ $e$ を保存する。\n",
    " \n",
    "QLearningPlayer クラスは QTable, Memory をもち、Q 値に従った行動決定と、局面の保存を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問2\n",
    "QLearningPlayer クラスの action メソッドを完成させなさい。  \n",
    "また、Q 値を更新する train_q_table 関数を完成させ、エージェントを訓練しなさい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QTable クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.table = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def s2key(s):\n",
    "        return np.str(s)\n",
    "\n",
    "    def getitem(self, s, a=None):\n",
    "        key = self.s2key(s)\n",
    "\n",
    "        if key not in self.table:\n",
    "            self.table[key] = np.zeros(9, dtype=np.float32)\n",
    "\n",
    "        if a is None:\n",
    "            return self.table[key]\n",
    "        else:\n",
    "            return self.table[key][a]\n",
    "\n",
    "    def setitem(self, s, a, value):\n",
    "        key = self.s2key(s)\n",
    "\n",
    "        if key not in self.table:\n",
    "            self.table[key] = np.zeros(9, dtype=np.float32)\n",
    "\n",
    "        self.table[key][a] = value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "\n",
    "    def __init__(self, size=10):\n",
    "        self.size = size\n",
    "        self.memory = np.empty((size, 21), dtype=np.float32)\n",
    "        self.counter = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(self.size, self.counter)\n",
    "\n",
    "    def read(self, ind):\n",
    "        s = self.memory[ind, :9].astype(np.int32)\n",
    "        a = self.memory[ind, 9].astype(np.int32)\n",
    "        s_dash = self.memory[ind, 10:19].astype(np.int32)\n",
    "        r = self.memory[ind, 19]\n",
    "        e = self.memory[ind, 20]\n",
    "        return s, a, s_dash, r, e\n",
    "\n",
    "    def write(self, ind, s, a, s_dash, r, e):\n",
    "        self.memory[ind, :9] = s\n",
    "        self.memory[ind, 9] = a\n",
    "        self.memory[ind, 10:19] = s_dash\n",
    "        self.memory[ind, 19] = r\n",
    "        self.memory[ind, 20] = e\n",
    "\n",
    "    def append(self, s, a, s_dash, r, e):\n",
    "        ind = self.counter % self.size\n",
    "        self.write(ind, s, a, s_dash, r, e)\n",
    "        self.counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player（Q 学習）クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningPlayer(object):\n",
    "\n",
    "    def __init__(self, name, q_table, memory,\n",
    "                 reward_win=1., reward_draw=0., reward_lose=-1.,\n",
    "                 eps=0.05):\n",
    "        self.name = name\n",
    "        self.reward_win = reward_win\n",
    "        self.reward_draw = reward_draw\n",
    "        self.reward_lose = reward_lose\n",
    "        self.eps = eps\n",
    "\n",
    "        self.q_table = q_table\n",
    "        self.memory = memory\n",
    "\n",
    "        self.s_last = None\n",
    "        self.a_last = None\n",
    "\n",
    "        self.record = []\n",
    "\n",
    "    def action(self, board):\n",
    "        s = board\n",
    "\n",
    "        # decide the action\n",
    "        if np.random.random() < self.eps:\n",
    "            # epsilon-greedy. ランダムな行動\n",
    "            a_candidate = np.arange(9)[board == EMPTY]\n",
    "            a = np.random.choice(a_candidate)\n",
    "        else:\n",
    "            # Q 値に従った行動\n",
    "            q = self.q_table.getitem(s)\n",
    "            q[board != EMPTY] = -np.inf\n",
    "            a = np.argmax(q)\n",
    "\n",
    "        # memorise state and action\n",
    "        if self.s_last is not None:\n",
    "            self.memory.append(self.s_last, self.a_last, s, 0, 0)\n",
    "        self.s_last = board\n",
    "        self.a_last = a\n",
    "\n",
    "        return a\n",
    "\n",
    "    def finalize(self, condition, board):\n",
    "        s = board\n",
    "        if condition == WIN:\n",
    "            r = self.reward_win\n",
    "        elif condition == DRAW:\n",
    "            r = self.reward_draw\n",
    "        else:\n",
    "            r = self.reward_lose\n",
    "        self.memory.append(self.s_last, self.a_last, s, r, 1)\n",
    "\n",
    "        self.s_last = None\n",
    "        self.a_last = None\n",
    "\n",
    "        self.record.append(condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_table(q_table, memory, lr=0.1, gamma=0.95, n_epoch=1):\n",
    "    q_table_copy = copy.deepcopy(q_table)\n",
    "\n",
    "    for e in range(n_epoch):\n",
    "        for i in range(len(memory)):\n",
    "            s, a, s_dash, r, e = memory.read(i)\n",
    "            q_s_a = q_table.getitem(s, a)\n",
    "            if e == 0:\n",
    "                max_q_s_a_dash = np.max(q_table_copy.getitem(s_dash))\n",
    "            else:\n",
    "                max_q_s_a_dash = 0\n",
    "\n",
    "            # Q 値の更新\n",
    "            q_s_a_new = q_s_a + lr * (r + gamma * max_q_s_a_dash - q_s_a)\n",
    "            q_table.setitem(s, a, q_s_a_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = RandomPlayer('乱太郎')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = QTable()\n",
    "memory = Memory()\n",
    "\n",
    "q_learning = QLearningPlayer('Q太郎', q_table, memory, eps=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:34<00:00, 289.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(10000)):\n",
    "\n",
    "#     if i % 100 == 0:\n",
    "#         print(i, end=', ')\n",
    "\n",
    "    if np.random.random() > 0.5:\n",
    "        X = random\n",
    "        O = q_learning\n",
    "    else:\n",
    "        X = q_learning\n",
    "        O = random\n",
    "\n",
    "    game = TicTacToe(X, O)\n",
    "    game.main_loop(print_game=False)\n",
    "    train_q_table(q_table, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: \n",
      " 0 | 1 | 2 \n",
      "-----------\n",
      " 3 | 4 | 5 \n",
      "-----------\n",
      " 6 | 7 | 8\n",
      "\n",
      "Q太郎(X)'s turn.\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |  \n",
      "\n",
      "乱太郎(O)'s turn.\n",
      " X |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |  \n",
      "\n",
      "Q太郎(X)'s turn.\n",
      " X |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      " O |   |  \n",
      "\n",
      "乱太郎(O)'s turn.\n",
      " X | X |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      " O |   |  \n",
      "\n",
      "Q太郎(X)'s turn.\n",
      " X | X |   \n",
      "-----------\n",
      " O |   |   \n",
      "-----------\n",
      " O |   |  \n",
      "\n",
      " X | X | X \n",
      "-----------\n",
      " O |   |   \n",
      "-----------\n",
      " O |   |  \n",
      "\n",
      "GAME is OVER!\n",
      "WINNER: Q太郎\n"
     ]
    }
   ],
   "source": [
    "game = TicTacToe(q_learning, random)\n",
    "game.main_loop(print_game=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff581ad9630>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGxNJREFUeJzt3XmUXOV95vHvr/Ze1a2l29pAAiRjGS9AG4M32iw2MGPI4nEgExs8ODqTCZPYnskcGGfsBB87cRa8TLCNJrbj+CTI2PFxNBjDeKGDVxCLjZGERCM2CRktqFu91vqbP+p2U91Sq6qlaqrv7edzTh/ufe9bt963bvPo9nvvfcvcHRERiZZYoxsgIiL1p3AXEYkghbuISAQp3EVEIkjhLiISQQp3EZEIqhruZvZlM9tvZo/NsN3M7HNm1m9mj5rZOfVvpoiIzEYtZ+7/AFx2nO2XA+uCn43AF06+WSIicjKqhru73we8eJwqVwH/6GU/BzrMbHm9GigiIrOXqMM+VgLPVazvCcr2Ta9oZhspn93T1NR07urVq0/oDUulErHYwrpcoD4vDOrzwnAyfd61a9dBd19WrV49wr1m7r4J2ATQ09PjDz744Antp6+vj97e3jq2bP5TnxcG9XlhOJk+m9kztdSrxz+Xe4HKU/BVQZmIiDRIPcJ9C/C+4K6Z84FBdz9qSEZERF4+VYdlzOx2oBdYamZ7gI8BSQB3/yJwF3AF0A+MAu+fq8aKiEhtqoa7u19TZbsDf1i3FomIyElbWJeoRUQWCIW7iEgEKdxFRCJI4S4iEkEKdxGRCFK4i4hEkMJdRCSCFO4iIhGkcBcRiSCFu4hIBCncRUQiSOEuIhJBCncRkQhSuIuIRJDCXUQkghTuIiIRpHAXEYkghbuISAQp3EVEIkjhLiISQQp3EZEIUriLiESQwl1EJIIU7iIiEaRwFxGJIIW7iEgEKdxFRCJI4S4iEkEKdxGRCFK4i4hEkMJdRCSCFO4iIhGkcBcRiSCFu4hIBCncRUQiqKZwN7PLzGynmfWb2Y3H2H6Kmd1rZo+Y2aNmdkX9myoiIrWqGu5mFgduBS4HNgDXmNmGadX+FLjD3c8GrgY+X++GiohI7Wo5cz8P6Hf33e6eAzYDV02r40B7sLwIeL5+TRQRkdkydz9+BbN3A5e5+weC9fcCb3T3GyrqLAf+H9AJtACXuPtDx9jXRmAjQHd397mbN28+oUYPDw/T2tp6Qq8NK/V5YVCfF4aT6fPb3/72h9y9p1q9xAnt/WjXAP/g7n9rZhcAXzOzs9y9VFnJ3TcBmwB6enq8t7f3hN6sr6+PE31tWKnPC4P6vDC8HH2uZVhmL7C6Yn1VUFbpeuAOAHf/GZABltajgSIiMnu1hPtWYJ2ZrTWzFOULplum1XkWuBjAzF5FOdwP1LOhIiJSu6rh7u4F4AbgHmAH5btitpnZzWZ2ZVDtvwG/b2a/BG4HrvNqg/kiIjJnahpzd/e7gLumlX20Ynk78Ob6Nk1EZP5wd7KFEtlCiSNjeQ4OZxkYyzM4Wl5OJ+O4O2aGASV3xvNF8kUnmy9yZLzAeL7IeL7I+mSR3jlub70uqIqINFyuUCJXLJHNF8kWShwazjGaK3B4NEfMjIGxPAOjOXKFEgOjeQ6P5mnLJEglYrxwZJxMIk62UGQsX2Q8X2JwLM+LI+V9HBkrkCuWqjfiGMygNZ2gKRknk4zTtWruBzYU7iIybxRLzv6hcQpF5/mBMQ4O5xjO5hkYzVMoOUfG8xwZy/PMoVH2DY7TnIrTlIwzkiuyb3CMgdF8ze+VScbobE4xPF4O7a72NGO5Ek2pGM3JBOlkjEVNSU5d0kxLOkFbJkF7Jkk6EaO9Kcmy1jTtTUkWNSVZ2poiVywRM6NUchyImZFJxkglYiRjMWIxm3zvvr6++n940yjcReQo7j559jqSLVByJ2bGouYk7pBOxMgk4wBkC0WOjBV49sURnh8YZyxXJFcsEY8ZiZiRK5Ymy9zh4HCWg8M5BkZz7B0YY2BolPy991AoObliiWJp5rPaVDxGWybB6sXNbFjRzmi2wFi+yIpFGc49tYOutgyZZIxkvByqS1rStKYTLGpKUnKnszlFZ0uSdCJOKhHtqbUU7iIhNjia59BIdjIQJwK55I4BB4az7D4wwq+PjHNoOEsiFiMRN0ZzRXKFEp3NSdqbkgyOlc+IB8byHBjK8uvBcQrHCVmAZNxozyQ5NJKbVZtbUnG62jO0ZRKc+Yo2BpNZ1q1ZRSJmZJJxutvTJOIxVnU2sbQ1TVsmQUdzimTcSMVjmFn1NxGFu8jLbTRXYN/gOM8cGuHgUI7BsTx7B8bYse8Izx8co/WXPyIeg/ZMktFckSNj+cmzzEwyjgPD4+XXjOdrGwNe3JKiozkJQDZfmhxnfuKFIQbG8nQ2p1jUlGRxS4rVpzSzsrOJ1nSCTDJOazpOPBajUCxxeDRPPAbD4wXGCyWGxvMsa8vQ2VwevljR0TQ5pFFyJ19w0skYmUScdLLch3RiakCXH+h5dX0/ZFG4ixzPeL7I4dEci5qSGMZ4cKFucUuK5wfGePbFUdKJGAeGs4xkC/x6MMvzA2PsHRjjyHgedxgYy5GKxxgKxnYHx8rllZqScdZ3t9KRNro7MhRKzuBYntZ0gpUdTYzli8TMyBaKuMOy1jQXndnFsrY0y9rSxGMx3J1kPEZzKk48ZhjGktYUa5e2TA6hyMKhcJcFq1AssW9wnAPDWQ4OlUN5x74h9g+NM5ItsufwKM8Pjs96v0taUnS1Z2jPJEjEjZUdTZN3SyTiMbra0pyyuJlTlzSzuCUVnFWngImz2DfUu6uyACncJVRKJWc0X2T/kXFKwYW9sXyR5lSc5YuayBdLHBrJsefFUfYPZTk0nGU0X6RYdIayBRY1JTkwlGXb84M8vm+IoWxhyv47mpMsCcK2Z81iVi9uorM5xdB4ATNIJWKkE3EGRnN0taU5dUkLhVKJrrYM7ZkkXe1pnSXLvKBwl5fF4FieJ14Y4uFnD7NvcJxDwzniwQW0pw+OcHg0R65YYllrmpFcgeZUggMvjvG1p7eSjMd46uAI+WKJpw+NMNN1vnjMjnunxYSmZJxXLW/jqrNXsK6rjZUdTXQ0J1kWhLVIFCjc5aSM54v8+ImDvDA0zt7DYzQl4wxlCwyM5hgaL/DE/mH2DYwxkitOed2ipiRmMJItsLqzPDyRSsQYGM2TTJTHp2PA7oMjFEol1ixpIRmP8eYzlk6OMxtQKDnpRIxnDo0yli/S0ZRkUXOSrrbM5D3KmWT5trfmZJzhXIHmZJxEPNq3wYko3GVWdr0wxH27DvDws4d55tAoTx8cOSq4ofyASFMyzqtXLOINaxZzyuJmVnRkeMOaxSxtTZOMW9Vb2uZiWtT2TLKu+xOZrxTuC9DB4Sz/8tAe/uK7j08pb0snGMoW6GxO8r+vOYdCqcQt39vFo3sGSSdiZAtTb7t71fJ2el/ZxZWvX8GaJS20pOO0N5Wf4EsnNO4s0kgK93luYiKianbsO8K1X36A/UNZoPygyLHOqI9n4uLi4dE8v/el+6dsyxZKdDQnOX1ZKxed2cXlZ72C05YtrG/PEQkThfs8dGQ8z29//qc8sX94smx9Z4xC1wu8bf0yUokYDzz1Ih/bso3hbJ7nXhw7ah/HC/Z1Xa38xtkrue5Na2hJv/Qr8PxAecz8yQPDXPeVrfzOG1bzoUvX05KK66lAkZBRuDfYeL7IH93+CCWH7+94YcZ6uw6X+MA/Pjjj9tOWtnDbe89lXXcbQ+N5xvJFSiXobk/jDuOFIk3J44f0io4mAHpaFvPYn7/zxDslIg2ncG+A8XyRn+0+xPu/snXGOn/49tN527plnLd2MQeGs/zlHfdx797ykAnAW9ct5T09q3l0zwAfvGT9lDPwtkyStooLh2bQnNKhFllI9H98HfXvH+KSW+6bXH/gf17MZ3/wBKcuaeaTdz3OGV2t9FcMtUx4yxlLGcoW2PTec1namiYem3p23dWW4crTU9xyfe9Rr33X61bUvR8iEn4K91kqFEs8/OwAhWKJ7kUZTg8uKn77kb188Ou/mFL3vE/+YMr69GD//offxhldbXPbYBFZkBTuNXjk2cP85ud/WlPd6960hkwyzhf/7ckp5e9/8xr2D2XZ+NbTeN3qjrlopojIpAUT7r/aM8j/+dFu/vo/vLbqPdg79h1hRUcT7/niz9j5wlDN7/Hl63q46MxuAG68/MyTaq+IyMlYEOG+5sbvTC5v+eXzPP2X/27Gul/68VN8/M7tx9zW0Zykd/0yPnP12QDsOTzKF/qeZHFLine9bgXruzXEIiLzQ+TD/emDI0eVvfsLP2Vddxt/8VuvYf+RcR7dM8ibzljCzf93O5u3Pjel7oXrl/Hxq87ilCXNR+1nVWczn/jN18xZ20VETlTowv0n/Qf5ox+O8rMLClNu/5tJ79/0HVX24DOHefCZw9z+wLPHfM1/vvB0/uDC01nUrHlIRCScQjc13qfufpwjOT/mLYXH0/+Jy3n4f11atd5nfuf13Hj5mQp2EQm10J25z0auYqKrRDzG4pYUT37yCp4+NMLFf/tvk9t+dtNFbH7gOW646AySmgpWRCIg0uG+eWt52OWTFePi8Zhx+rJWHvrTSxjJFifH0j906fqGtFFEZC6ENtyrf98OfPRftwHw2lWLjtq2pDXNEk1qKCIRFboxiFrnJrzpW7+aXH71iva5aYyIyDwV2jP3mXzzoT3892/8cnL9HRu6NV2tiCw4oTtzr6Yy2AFue++5DWqJiEjjhDbc3Y8edS+Vppade2qnztpFZEEKbbhP5+587efPTCn7u989u0GtERFprNCOuU8/I//e9hf42Jby3TGfvfr1nLK4meWLmhrRNBGRhovMmfu+wfHJ5VQ8xtmndDawNSIijVVTuJvZZWa208z6zezGGeq8x8y2m9k2M/vn+jbzaNPH3HdVTM3b+8quuX57EZF5reqwjJnFgVuBS4E9wFYz2+Lu2yvqrANuAt7s7ofNbO7SdYYLpP90/0uTgDWljj9fu4hI1NVy5n4e0O/uu909B2wGrppW5/eBW939MIC7769vM48vX3xpDpkHPnLxy/nWIiLzUi0XVFcClZOc7wHeOK3OegAz+wkQB/7M3e+eviMz2whsBOju7qavr2/WDR46MgbAww8/zODu8hn6dXe/NGf79od+zrG/aiPchoeHT+jzCjP1eWFQn+dGve6WSQDrgF5gFXCfmb3G3QcqK7n7JmATQE9Pj/f29s76jT697ScwOMBpZ76GVCLGm85YCne/9E1LJ7LPMOjr64ts32aiPi8M6vPcqGVYZi+wumJ9VVBWaQ+wxd3z7v4UsIty2NfdxIj79V/dyu/+/f2M54uT26694NS5eEsRkdCpJdy3AuvMbK2ZpYCrgS3T6nyb8lk7ZraU8jDN7jq2c9LEPTITD6N++nu7Jrf92ZWvnou3FBEJnarh7u4F4AbgHmAHcIe7bzOzm83syqDaPcAhM9sO3Av8ibsfmqtGV7rtvpf+DdFUAyIiZTWNubv7XcBd08o+WrHswIeDn4bofeWyRr21iMi8E7onVGc6N/9ExbctiYgsdKEL95l0taUb3QQRkXkjMuGuL7YWEXmJElFEJIJCF+66IUZEpLrQhXuls1aWv/h6XVdrg1siIjK/hPbLOgBWLGrizv/61kY3Q0Rk3gn1mXvlbJAiIvKSUIf7vTsPNLoJIiLzUqjDXUREjk3hLiISQaEL98o7Ic98RVvD2iEiMp+FLtwrtWVCfbOPiMicCXW4n6H720VEjinU4f6xd+nLOUREjiV04T40XphcziTjDWyJiMj8Fbpwf2L/cKObICIy74Uu3EVEpDqFu4hIBCncRUQiSOEuIhJBCncRkQhSuIuIRJDCXUQkghTuIiIRpHAXEYkghbuISAQp3EVEIkjhLiISQQp3EZEIUriLiESQwl1EJIIU7iIiEaRwFxGJIIW7iEgE1RTuZnaZme00s34zu/E49X7bzNzMeurXRBERma2q4W5mceBW4HJgA3CNmW04Rr024I+B++vdSBERmZ1aztzPA/rdfbe754DNwFXHqPdx4FPAeB3bJyIiJyBRQ52VwHMV63uAN1ZWMLNzgNXu/h0z+5OZdmRmG4GNAN3d3fT19c26wZVO9vVhMjw8vKD6C+rzQqE+z41awv24zCwG3AJcV62uu28CNgH09PR4b2/v7N/w7u9MLp7Q60Oqr69vQfUX1OeFQn2eG7UMy+wFVlesrwrKJrQBZwF9ZvY0cD6wRRdVRUQap5Zw3wqsM7O1ZpYCrga2TGx090F3X+rua9x9DfBz4Ep3f3BOWiwiIlVVDXd3LwA3APcAO4A73H2bmd1sZlfOdQNFRGT2ahpzd/e7gLumlX10hrq9J9+smb3p9CX89MlDc/kWIiKhF7onVM98RTsAbemTvhYsIhJZoQt3ERGpLrTh7o1ugIjIPBa6cF++KAPAzVe9usEtERGZv0IX7sm4AfD2V3Y1uCUiIvNX6MJdRESqU7iLiESQwl1EJIJCF+66S0ZEpLrQhfsEs0a3QERk/gptuIuIyMwU7iIiEaRwFxGJIIW7iEgEKdxFRCJI4S4iEkEKdxGRCApduLueYhIRqSp04T7B0FNMIiIzCW24i4jIzBTuIiIRpHAXEYkghbuISAQp3EVEIkjhLiISQQp3EZEICl246xkmEZHqQhfuk/QMk4jIjMIb7iIiMiOFu4hIBCncRUQiSOEuIhJBCncRkQhSuIuIRFBN4W5ml5nZTjPrN7Mbj7H9w2a23cweNbMfmNmp9W+qiIjUqmq4m1kcuBW4HNgAXGNmG6ZVewTocffXAt8E/qreDZ3g+iomEZGqajlzPw/od/fd7p4DNgNXVVZw93vdfTRY/Tmwqr7NPJrpISYRkRklaqizEniuYn0P8Mbj1L8e+O6xNpjZRmAjQHd3N319fbW1ssKTT+cB+PGPfkxzcuEk/PDw8Al9XmGmPi8M6vPcqCXca2Zmvwf0ABcea7u7bwI2AfT09Hhvb++s36P/R7vh8R285a1voT2TPInWhktfXx8n8nmFmfq8MKjPc6OWcN8LrK5YXxWUTWFmlwAfAS5092x9miciIieiljH3rcA6M1trZingamBLZQUzOxu4DbjS3ffXv5kiIjIbVcPd3QvADcA9wA7gDnffZmY3m9mVQbW/BlqBb5jZL8xsywy7ExGRl0FNY+7ufhdw17Syj1YsX1LndomIyEnQE6oiIhGkcBcRiaDQhvvCucNdRGT2QhvuIiIyM4W7iEgEKdxFRCJI4S4iEkEKdxGRCFK4i4hEkMJdRCSCQhfu+iImEZHqQhfuE0xfxSQiMqPQhruIiMxM4S4iEkEKdxGRCFK4i4hEkMJdRCSCFO4iIhGkcBcRiaDQhbujp5hERKoJXbhP0CNMIiIzC224i4jIzBTuIiIRpHAXEYkghbuISAQp3EVEIkjhLiISQQp3EZEICl2465uYRESqC124T9AXMYmIzCy04S4iIjNTuIuIRJDCXUQkghTuIiIRpHAXEYmgmsLdzC4zs51m1m9mNx5je9rMvh5sv9/M1tS7oSIiUruq4W5mceBW4HJgA3CNmW2YVu164LC7nwF8GvhUvRsqIiK1q+XM/Tyg3913u3sO2AxcNa3OVcBXg+VvAheb6U50EZFGSdRQZyXwXMX6HuCNM9Vx94KZDQJLgIOVlcxsI7AxWB02s50n0mhgacunpu57AVgK6vMCoD4vDCfT51NrqVRLuNeNu28CNp3sfszsQXfvqUOTQkN9XhjU54Xh5ehzLcMye4HVFeurgrJj1jGzBLAIOFSPBoqIyOzVEu5bgXVmttbMUsDVwJZpdbYA1wbL7wZ+6K4pvkREGqXqsEwwhn4DcA8QB77s7tvM7GbgQXffAnwJ+JqZ9QMvUv4HYC6d9NBOCKnPC4P6vDDMeZ9NJ9giItGjJ1RFRCJI4S4iEkGhC/dqUyGEhZmtNrN7zWy7mW0zsz8Oyheb2ffM7Ingv51BuZnZ54J+P2pm51Ts69qg/hNmdu1M7zlfmFnczB4xszuD9bXBtBX9wTQWqaB8xmktzOymoHynmb2zMT2pjZl1mNk3zexxM9thZhdE/Tib2YeC3+vHzOx2M8tE7Tib2ZfNbL+ZPVZRVrfjambnmtmvgtd8btYPhrp7aH4oX9B9EjgNSAG/BDY0ul0n2JflwDnBchuwi/L0Dn8F3BiU3wh8Kli+AvguYMD5wP1B+WJgd/DfzmC5s9H9q9L3DwP/DNwZrN8BXB0sfxH4g2D5vwBfDJavBr4eLG8Ijn0aWBv8TsQb3a/j9PerwAeC5RTQEeXjTPmhxqeAporje13UjjPwNuAc4LGKsrodV+CBoK4Fr718Vu1r9Ac0yw/zAuCeivWbgJsa3a469e1fgUuBncDyoGw5sDNYvg24pqL+zmD7NcBtFeVT6s23H8rPSfwAuAi4M/jFPQgkph9jyndoXRAsJ4J6Nv24V9abbz+Un/l4iuDmhenHL4rHmZeeWF8cHLc7gXdG8TgDa6aFe12Oa7Dt8YryKfVq+QnbsMyxpkJY2aC21E3wZ+jZwP1At7vvCzb9GugOlmfqe9g+k88A/wMoBetLgAF3LwTrle2fMq0FMDGtRZj6vBY4AHwlGIr6ezNrIcLH2d33An8DPAvso3zcHiLax3lCvY7rymB5ennNwhbukWNmrcC/AB909yOV27z8T3Zk7lU1s38P7Hf3hxrdlpdRgvKf7l9w97OBEcp/rk+K4HHupDyZ4FpgBdACXNbQRjVAo49r2MK9lqkQQsPMkpSD/Z/c/VtB8QtmtjzYvhzYH5TP1PcwfSZvBq40s6cpzy56EfBZoMPK01bA1PbPNK1FmPq8B9jj7vcH69+kHPZRPs6XAE+5+wF3zwPfonzso3ycJ9TruO4NlqeX1yxs4V7LVAihEFz5/hKww91vqdhUOZXDtZTH4ifK3xdcdT8fGAz+/LsHeIeZdQZnTO8IyuYdd7/J3Ve5+xrKx+6H7v4fgXspT1sBR/f5WNNabAGuDu6yWAuso3zxad5x918Dz5nZK4Oii4HtRPg4Ux6OOd/MmoPf84k+R/Y4V6jLcQ22HTGz84PP8H0V+6pNoy9InMAFjCso31nyJPCRRrfnJPrxFsp/sj0K/CL4uYLyWOMPgCeA7wOLg/pG+UtTngR+BfRU7Os/Af3Bz/sb3bca+9/LS3fLnEb5f9p+4BtAOijPBOv9wfbTKl7/keCz2Mks7yJoQF9fDzwYHOtvU74rItLHGfhz4HHgMeBrlO94idRxBm6nfE0hT/kvtOvreVyBnuDzexL4O6ZdlK/2o+kHREQiKGzDMiIiUgOFu4hIBCncRUQiSOEuIhJBCncRkQhSuIuIRJDCXUQkgv4/ytqyS1P3cxkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 勝率の変化\n",
    "wininig_Q = np.array(q_learning.record) == WIN\n",
    "\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot(np.cumsum(wininig_Q) / (np.arange(len(wininig_Q)) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問２では、Q 値を状態 $s$ を Key とする辞書で表現していた。  \n",
    "○×ゲームのように状態数に限りがある環境ではこれでも問題ないが、例えば TV ゲームのような複雑な環境では、状態数が爆発的に増加してしまい、すべての状態について Q 値を保持することが不可能になってしまう。\n",
    "\n",
    "この問題に対処する方法として、 **Q 値をニューラルネットワークで近似する** というものがある。  \n",
    "すなわち、状態 $s$ を入力とし、各行動 $a$ についての Q 値を出力するニューラルネットワークモデルを構築し、先の更新式に従ってモデルを訓練するのである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問3\n",
    "\n",
    "問２の QTable クラスの代わりに、ニューラルネットワークを用いた QFunction クラスを実装し、モデルを訓練せよ。  \n",
    "具体的には、\n",
    "$$\n",
    "loss = \\frac{1}{2}(Q^{new}_\\theta(s, a) - Q_\\theta(s, a))^2\n",
    "$$\n",
    "を最小化するように、QFunction のパラメタ $\\theta$ を更新してゆけば良い。\n",
    "\n",
    "#### 注意\n",
    "- ニューラルネットワークフレームワークを使用して良い\n",
    "- 本問の正解／不正解は選抜においてあまり重視しない（ただしインターンでは深層強化学習を用いたゲーム AI の作成に取り組んでいただく予定なので、挑戦してもらえると幸いです。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunction(chainer.Chain):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(QFunction, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l0 = L.Linear(9, 128)\n",
    "            self.l1 = L.Linear(128, 128)\n",
    "            self.l2 = L.Linear(128, 9)\n",
    "\n",
    "    def __call__(self, s):\n",
    "        h = F.elu(self.l0(s))\n",
    "        h = F.elu(self.l1(h))\n",
    "        qs = F.softmax(self.l2(h))\n",
    "        return qs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNQLearningPlayer(object):\n",
    "\n",
    "    def __init__(self, name, q_function, memory,\n",
    "                 reward_win=1., reward_draw=0., reward_lose=-1.,\n",
    "                 eps=0.05):\n",
    "        self.name = name\n",
    "        self.reward_win = reward_win\n",
    "        self.reward_draw = reward_draw\n",
    "        self.reward_lose = reward_lose\n",
    "        self.eps = eps\n",
    "\n",
    "        self.q_function = q_function\n",
    "        self.memory = memory\n",
    "\n",
    "        self.s_last = None\n",
    "        self.a_last = None\n",
    "\n",
    "        self.record = []\n",
    "\n",
    "    def action(self, board):\n",
    "        s = board\n",
    "\n",
    "        # decide the action\n",
    "        if np.random.random() < self.eps:\n",
    "            # epsilon-greedy. ランダムな行動\n",
    "            a_candidate = np.arange(9)[board == EMPTY]\n",
    "            a = np.random.choice(a_candidate)\n",
    "        else:\n",
    "            # Q 値に従った行動\n",
    "            x = s.reshape(1, 9).astype(np.float32)\n",
    "            q = self.q_function(x).data[0]\n",
    "            q[board != EMPTY] = -np.inf\n",
    "            a = np.argmax(q)\n",
    "\n",
    "        # memorise state and action\n",
    "        if self.s_last is not None:\n",
    "            self.memory.append(self.s_last, self.a_last, s, 0, 0)\n",
    "        self.s_last = board\n",
    "        self.a_last = a\n",
    "\n",
    "        return a\n",
    "\n",
    "    def finalize(self, condition, board):\n",
    "        s = board\n",
    "        if condition == WIN:\n",
    "            r = self.reward_win\n",
    "        elif condition == DRAW:\n",
    "            r = self.reward_draw\n",
    "        else:\n",
    "            r = self.reward_lose\n",
    "        self.memory.append(self.s_last, self.a_last, s, r, 1)\n",
    "\n",
    "        self.s_last = None\n",
    "        self.a_last = None\n",
    "\n",
    "        self.record.append(condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_function(q_function, memory, optimizer,\n",
    "                     batch_size=32, gamma=0.9, n_epoch=1):\n",
    "    q_function_copy = copy.deepcopy(q_function)\n",
    "    sum_loss = 0\n",
    "    for e in range(n_epoch):\n",
    "        perm = np.random.permutation(len(memory))\n",
    "        for i in range(0, len(memory), batch_size):\n",
    "            s, a, s_dash, r, e = memory.read(perm[i:i+batch_size])\n",
    "            x = s.astype(np.float32)\n",
    "            x_dash = s_dash.astype(np.float32)\n",
    "\n",
    "            q_s = q_function(x)\n",
    "            q_s_dash = q_function_copy(x_dash).data\n",
    "\n",
    "            max_q_s_a_dash = np.max(q_s_dash, axis=1)\n",
    "            max_q_s_a_dash[e == 1] == 0\n",
    "\n",
    "            t = q_s.data.copy()\n",
    "            t[np.arange(len(t)), a] += r + gamma * \\\n",
    "                max_q_s_a_dash - t[np.arange(len(t)), a]\n",
    "\n",
    "            loss = F.mean_absolute_error(q_s, t)\n",
    "\n",
    "            q_function.cleargrads()\n",
    "            loss.backward()\n",
    "            optimizer.update()\n",
    "\n",
    "            sum_loss += loss.data\n",
    "    return sum_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_function = QFunction()\n",
    "memory = Memory(size=128)\n",
    "\n",
    "nn_q_learning = NNQLearningPlayer('Q太郎', q_function, memory, eps=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.optimizers.adam.Adam at 0x7ff53c1d6f98>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(q_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:26<00:00, 68.08it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(10000)):\n",
    "\n",
    "#     if i % 100 == 0:\n",
    "#         print(i, end=', ')\n",
    "\n",
    "    if np.random.random() > 0.5:\n",
    "        X = random\n",
    "        O = nn_q_learning\n",
    "    else:\n",
    "        X = nn_q_learning\n",
    "        O = random\n",
    "\n",
    "    game = TicTacToe(X, O)\n",
    "    game.main_loop(print_game=False)\n",
    "    loss = train_q_function(q_function, memory, optimizer)\n",
    "\n",
    "#     if i % 100 == 0:\n",
    "#         print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: \n",
      " 0 | 1 | 2 \n",
      "-----------\n",
      " 3 | 4 | 5 \n",
      "-----------\n",
      " 6 | 7 | 8\n",
      "\n",
      "Q太郎(X)'s turn.\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |  \n",
      "\n",
      "乱太郎(O)'s turn.\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   | X |  \n",
      "\n",
      "Q太郎(X)'s turn.\n",
      "   | O |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   | X |  \n",
      "\n",
      "乱太郎(O)'s turn.\n",
      "   | O |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   | X | X\n",
      "\n",
      "Q太郎(X)'s turn.\n",
      "   | O |   \n",
      "-----------\n",
      " O |   |   \n",
      "-----------\n",
      "   | X | X\n",
      "\n",
      "   | O |   \n",
      "-----------\n",
      " O |   |   \n",
      "-----------\n",
      " X | X | X\n",
      "\n",
      "GAME is OVER!\n",
      "WINNER: Q太郎\n"
     ]
    }
   ],
   "source": [
    "game = TicTacToe(nn_q_learning, random)\n",
    "game.main_loop(print_game=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff52f74ea20>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 勝率の変化\n",
    "wininig_Q = np.array(nn_q_learning.record) == WIN\n",
    "\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot(np.cumsum(wininig_Q) / (np.arange(len(wininig_Q)) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 遊ぶ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: \n",
      " 0 | 1 | 2 \n",
      "-----------\n",
      " 3 | 4 | 5 \n",
      "-----------\n",
      " 6 | 7 | 8\n",
      "\n",
      "Q太郎(X)'s turn.\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |  \n",
      "\n",
      "YAMAUCHI(O)'s turn.\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   | X |  \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q太郎(X)'s turn.\n",
      "   |   |   \n",
      "-----------\n",
      "   | O |   \n",
      "-----------\n",
      "   | X |  \n",
      "\n",
      "YAMAUCHI(O)'s turn.\n",
      "   |   |   \n",
      "-----------\n",
      "   | O |   \n",
      "-----------\n",
      "   | X | X\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q太郎(X)'s turn.\n",
      "   |   |   \n",
      "-----------\n",
      "   | O |   \n",
      "-----------\n",
      " O | X | X\n",
      "\n",
      "YAMAUCHI(O)'s turn.\n",
      "   |   | X \n",
      "-----------\n",
      "   | O |   \n",
      "-----------\n",
      " O | X | X\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q太郎(X)'s turn.\n",
      "   |   | X \n",
      "-----------\n",
      "   | O | O \n",
      "-----------\n",
      " O | X | X\n",
      "\n",
      "YAMAUCHI(O)'s turn.\n",
      " X |   | X \n",
      "-----------\n",
      "   | O | O \n",
      "-----------\n",
      " O | X | X\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X |   | X \n",
      "-----------\n",
      " O | O | O \n",
      "-----------\n",
      " O | X | X\n",
      "\n",
      "GAME is OVER!\n",
      "WINNER: YAMAUCHI\n"
     ]
    }
   ],
   "source": [
    "nn_q_learning = NNQLearningPlayer('Q太郎', q_function, memory, eps=0.)\n",
    "game = TicTacToe(nn_q_learning, HumanPlayer('YAMAUCHI'))\n",
    "game.main_loop(print_game=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...まだ弱い\n",
    "\n",
    "### 自分自身との対局"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory0 = Memory(size=100)\n",
    "memory1 = Memory(size=100)\n",
    "nn_q_learning0 = NNQLearningPlayer('Q太郎', q_function, memory0, eps=0.05)\n",
    "nn_q_learning1 = NNQLearningPlayer('Q次郎', q_function, memory1, eps=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [01:23<00:00, 36.06it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(3000)):\n",
    "\n",
    "#     if i % 100 == 0:\n",
    "#         print(i, end=', ')\n",
    "\n",
    "    X = nn_q_learning0\n",
    "    O = nn_q_learning1\n",
    "\n",
    "    game = TicTacToe(X, O)\n",
    "    game.main_loop(print_game=False)\n",
    "    loss = train_q_function(q_function, memory0, optimizer)\n",
    "    loss = train_q_function(q_function, memory1, optimizer)\n",
    "\n",
    "#     if i % 100 == 0:\n",
    "#         print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### もう一度遊ぶ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: \n",
      " 0 | 1 | 2 \n",
      "-----------\n",
      " 3 | 4 | 5 \n",
      "-----------\n",
      " 6 | 7 | 8\n",
      "\n",
      "YAMAUCHI(X)'s turn.\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |   \n",
      "-----------\n",
      "   |   |  \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q太郎(O)'s turn.\n",
      "   |   |   \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      "   |   |  \n",
      "\n",
      "YAMAUCHI(X)'s turn.\n",
      " O |   |   \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      "   |   |  \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q太郎(O)'s turn.\n",
      " O |   |   \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      "   | X |  \n",
      "\n",
      "YAMAUCHI(X)'s turn.\n",
      " O |   |   \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      " O | X |  \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " O | X |   \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      " O | X |  \n",
      "\n",
      "GAME is OVER!\n",
      "WINNER: YAMAUCHI\n"
     ]
    }
   ],
   "source": [
    "nn_q_learning = NNQLearningPlayer('Q太郎', q_function, memory, eps=0.)\n",
    "game = TicTacToe(HumanPlayer('YAMAUCHI'), nn_q_learning)\n",
    "game.main_loop(print_game=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
