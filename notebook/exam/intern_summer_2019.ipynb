{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 株式会社ALBERT 2019年度夏季インターン選抜課題\n",
    "## 回答条件・提出物について\n",
    "### 条件\n",
    "- 回答には python 3.6 以上を用いること．\n",
    "- 使用ライブラリに制限は設けない．\n",
    "- 回答期限は１週間です．全問解けている必要はありません．可能な範囲で回答し提出してください．\n",
    "\n",
    "### 提出物\n",
    "- 回答を記述した本ノートブック．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題\n",
    "強化学習とは、 **エージェント (agent)** の行動決定問題を扱う機械学習分野の一領域である。  \n",
    "エージェントは周囲の環境の **状態 (state)** を観測し **行動 (action)** を選択する。  \n",
    "その際、環境から得られる **報酬 (reward)** を最大化するように行動指針を更新することで、適切な行動戦略を学習する。\n",
    "\n",
    "本課題では強化学習の一種である **Q 学習** を扱う。  \n",
    "Q 学習ではエージェントに **Q 値** という値を持たせ、これを各行動の有効性を示したものと考える。  \n",
    "エージェントは環境の中で行動を繰り返し、得られた報酬をもとに Q 値を調整してゆくことで、だんだんと賢くなっていくのである。\n",
    "\n",
    "さて、以下にあるのは **○×ゲーム (Tic-tac-toe)** のプログラムコードである。○×ゲームのルールは次の通りである。\n",
    "1. 3x3 のマス目を用意する\n",
    "1. 先手・後手がそれぞれ交互に○と×をマス目に書き込んでいく\n",
    "1. 縦横斜めいずれか一列を先に自分のマークで埋めた方が勝ち\n",
    "\n",
    "今回の実装では、これを次のように実現している。\n",
    "1. TicTacToe クラスは、Player クラスを内部に持つ\n",
    "1. TicTacToe クラスは、先手後手交互に Player クラスの action メソッドを呼ぶ\n",
    "1. action メソッドの戻り値をうけて TicTacToe クラスはゲームを進め、勝敗が決すれば Player クラスの finalize メソッドを呼ぶ\n",
    "\n",
    "TicTacToe クラスは action に際し、盤面情報 (board) を Player に渡すが、その盤面情報は渡す相手に合わせて normalize されていることに注意されたい。  \n",
    "すなわち、渡す相手のマークを MY_MARK, 渡す相手の敵のマークを ENEMY_MARK に変換してから board 配列を渡している。\n",
    "\n",
    "本課題の目的は、この○×ゲームを Q 学習によって攻略することである。  \n",
    "問題文に従ってプログラムを実装し、○×ゲームをプレイするエージェントを実装しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN = 1\n",
    "DRAW = 0\n",
    "LOSE = -1\n",
    "\n",
    "EMPTY = 0\n",
    "MY_MARK = 1\n",
    "ENEMY_MARK = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ゲーム盤クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe(object):\n",
    "\n",
    "    NUM_EMPTY = 0\n",
    "    NUM_X = 1\n",
    "    NUM_O = -1\n",
    "\n",
    "    def __init__(self, X, O):\n",
    "        self.X = X\n",
    "        self.O = O\n",
    "        self.board = np.full(9, self.NUM_EMPTY, dtype=np.int32)\n",
    "        self.num2player = {self.NUM_X: self.X,\n",
    "                           self.NUM_O: self.O}\n",
    "        self.num2mark = {self.NUM_EMPTY: ' ',\n",
    "                         self.NUM_X: 'X',\n",
    "                         self.NUM_O: 'O'}\n",
    "\n",
    "    def opposite_turn(self, turn):\n",
    "        if turn == self.NUM_X:\n",
    "            return self.NUM_O\n",
    "        else:\n",
    "            return self.NUM_X\n",
    "\n",
    "    def normalized_board(self, turn):\n",
    "        board = np.full(9, EMPTY, dtype=np.int32)\n",
    "        board[self.board == turn] = MY_MARK\n",
    "        board[self.board == self.opposite_turn(turn)] = ENEMY_MARK\n",
    "        return board\n",
    "\n",
    "    def move_and_judge(self, pos, turn):\n",
    "        winner = None\n",
    "        loser = None\n",
    "        game_is_over = False\n",
    "\n",
    "        winning_conditions = tuple([\n",
    "            [[0, 1, 2], [3, 4, 5], [6, 7, 8],\n",
    "             [0, 3, 6], [1, 4, 7], [2, 5, 8],\n",
    "             [0, 4, 8], [2, 4, 6]]\n",
    "        ])\n",
    "\n",
    "        if self.board[pos] != self.NUM_EMPTY:\n",
    "            game_is_over = True\n",
    "            winner = self.opposite_turn(turn)\n",
    "            loser = turn\n",
    "            return game_is_over, winner, loser\n",
    "\n",
    "        # update board.\n",
    "        self.board[pos] = turn\n",
    "\n",
    "        if not (self.board == self.NUM_EMPTY).any():\n",
    "            game_is_over = True\n",
    "\n",
    "        if (self.board[winning_conditions] == turn).all(axis=1).any():\n",
    "            winner = turn\n",
    "            loser = self.opposite_turn(turn)\n",
    "            game_is_over = True\n",
    "\n",
    "        return game_is_over, winner, loser\n",
    "\n",
    "    def print_board(self):\n",
    "        marks = [self.num2mark[num] for num in self.board]\n",
    "        board_text = (\n",
    "            ' {} | {} | {} \\n-----------\\n'\n",
    "            ' {} | {} | {} \\n-----------\\n'\n",
    "            ' {} | {} | {}'.format(*marks))\n",
    "        print(board_text + '\\n')\n",
    "\n",
    "    def print_actions(self):\n",
    "        board_text = (\n",
    "            ' {} | {} | {} \\n-----------\\n'\n",
    "            ' {} | {} | {} \\n-----------\\n'\n",
    "            ' {} | {} | {}'.format(*np.arange(9)))\n",
    "        print('actions: \\n' + board_text + '\\n')\n",
    "\n",
    "    def main_loop(self, print_game=False):\n",
    "        turn = self.NUM_X\n",
    "        game_is_over = False\n",
    "\n",
    "        if print_game:\n",
    "            self.print_actions()\n",
    "\n",
    "        while not game_is_over:\n",
    "            if print_game:\n",
    "                print('{}({})\\'s turn.'.format(\n",
    "                    self.num2player[turn].name, self.num2mark[turn]))\n",
    "                self.print_board()\n",
    "\n",
    "            player = self.num2player[turn]\n",
    "\n",
    "            normalized_board = self.normalized_board(turn)\n",
    "            pos = player.action(normalized_board)\n",
    "            game_is_over, winner, loser = self.move_and_judge(pos, turn)\n",
    "\n",
    "            turn = self.opposite_turn(turn)\n",
    "\n",
    "        if print_game:\n",
    "            self.print_board()\n",
    "            print('GAME is OVER!')\n",
    "            if winner is None:\n",
    "                print('DRAW')\n",
    "            else:\n",
    "                print('WINNER: {}'.format(self.num2player[winner].name))\n",
    "\n",
    "        if winner is None:  # DRAW\n",
    "            self.X.finalize(DRAW, self.normalized_board(self.NUM_X))\n",
    "            self.O.finalize(DRAW, self.normalized_board(self.NUM_O))\n",
    "        else:\n",
    "            self.num2player[winner].finalize(\n",
    "                WIN, self.normalized_board(winner))\n",
    "            self.num2player[loser].finalize(\n",
    "                LOSE, self.normalized_board(loser))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player（人間）クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer(object):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def action(self, board):\n",
    "        a = int(input())\n",
    "        return a\n",
    "\n",
    "    def finalize(self, condition, board):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行例\n",
    "human1 = HumanPlayer('YAMAUCHI')\n",
    "human2 = HumanPlayer('KATAGIRI')\n",
    "\n",
    "game = TicTacToe(human1, human2)\n",
    "game.main_loop(print_game=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問１\n",
    "board を受け取って、可能なマス目をランダムに選んで行動する RandomPlayer クラスをつくりたい。  \n",
    "以下の RandomPlayer クラスの action メソッドを完成させ、RandomPlayer 同士の対局を実行しなさい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player（ランダム）クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer(object):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def action(self, board):\n",
    "        # ここを埋める（問１）\n",
    "\n",
    "    def finalize(self, condition, board):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random1 = RandomPlayer('乱太郎')\n",
    "random2 = RandomPlayer('乱次郎')\n",
    "\n",
    "game = TicTacToe(random1, random2)\n",
    "game.main_loop(print_game=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 学習では、Q 値を用いて行動決定を行う。  \n",
    "Q 値はすべての (状態 $s$, 行動 $a$) の組に対して定義され、状態 $s$ において行動 $a$ を選択する有効性を示す。  \n",
    "ここで、状態 $s$ における 行動 $a$ の Q 値を $ Q(s, a) $ と書くことにする。\n",
    "\n",
    "Q 学習ではエージェントは状態 $s$ を観測し、Q 値を最大化する行動 $a$ を選択する。すなわち、状態 $s$ における行動 $ a(s) $ は、\n",
    "$$\n",
    "a(s) = argmax_a Q(s, a)\n",
    "$$\n",
    "で表される。  \n",
    "\n",
    "Q 値の更新は次のように行われる。\n",
    "$$\n",
    "Q^{new}(s, a) = Q(s, a) + lr \\cdot (r + \\gamma \\cdot max_a Q(s', a) - Q(s, a))\n",
    "$$\n",
    "ただし、 $lr$ は **学習率**、$\\gamma$ は **割引率**、 $r$ は 状態 $s$ において行動 $a$ を取った直後に得られた **報酬**、 $s'$ は 状態 $s$ において行動 $a$ を取った直後に観測された状況である。  \n",
    "\n",
    "更新式から明らかなように、状態 $s$ で行動 $a$ を取った際に正の $r$ が得られたとすると $Q(s, a)$ の値は大きくなる。  \n",
    "したがって、良い行動を取ったときに大きな $r$ を与え、間違ったときに小さな $r$ を与えるようしてやれば、エージェントの行動は改善されていくと考えられる。  \n",
    "どのような報酬 $r$ を与えるか（報酬設計）は強化学習一般において重要な問題である。\n",
    "\n",
    "さて、○×ゲームのような環境では、勝敗が決するまで、途中の各行動が良かったのかどうか判定できない。  \n",
    "そのため、勝敗に通じる最後の行動以外については適切な報酬を与えることが難しい。  \n",
    "Q 学習では状態 $s'$ における Q 値の（$a$ をいろいろ動かしたときの）最大値を更新時に加えることで、この問題に対処している。  \n",
    "次の時点での Q 値（の最大値）を現在の Q 値に伝播させることで、最後にのみ与えられる報酬の影響を、それ以前に遡って分配することができるのである。  \n",
    "（割引率 $\\gamma$ は、未来の報酬にどの程度影響を受けるかを制御するハイパーパラメタである。）\n",
    "\n",
    "ところで、上記のように Q 値を更新すると、エージェントは特定の行動ばかりを取るようになってしまうことが考えられる。  \n",
    "そうなるとエージェントは新しい局面に触れることがなくなり、学習が進まなくなってしまう場合がある。  \n",
    "それを防ぐ方法として、確率 $\\epsilon$ でランダムな行動を取る ε-greedy 法などのアルゴリズムが知られている。  \n",
    "ランダムに環境を探索させることで、エージェントにより詳しく環境について学んでもらおうという企みである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の実装では、Q 値を QTable クラスとして実装している。  \n",
    "QTable クラスは、状態 $s$ を Key，各行動 $a$ に対する Q 値（の配列）を Value とする辞書を内部に持ち、getitem メソッド、setitem メソッドによって Q 値を取り出したり書き換えたりできるようになっている。\n",
    "\n",
    "また、Q 値を更新するには、$s$, $a$, $s'$, $r$ を保存しておく必要があるので、これらを記憶する Memory クラスを用意した。  \n",
    "Memory クラスは $s$, $a$, $s'$, $r$ 及び、エピソード終了を表すフラグ $e$ を保存する。\n",
    " \n",
    "QLearningPlayer クラスは QTable, Memory をもち、Q 値に従った行動決定と、局面の保存を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問2\n",
    "QLearningPlayer クラスの action メソッドを完成させなさい。  \n",
    "また、Q 値を更新する train_q_table 関数を完成させ、エージェントを訓練しなさい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QTable クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.table = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def s2key(s):\n",
    "        return np.str(s)\n",
    "\n",
    "    def getitem(self, s, a=None):\n",
    "        key = self.s2key(s)\n",
    "\n",
    "        if key not in self.table:\n",
    "            self.table[key] = np.zeros(9, dtype=np.float32)\n",
    "\n",
    "        if a is None:\n",
    "            return self.table[key]\n",
    "        else:\n",
    "            return self.table[key][a]\n",
    "\n",
    "    def setitem(self, s, a, value):\n",
    "        key = self.s2key(s)\n",
    "\n",
    "        if key not in self.table:\n",
    "            self.table[key] = np.zeros(9, dtype=np.float32)\n",
    "\n",
    "        self.table[key][a] = value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "\n",
    "    def __init__(self, size=10):\n",
    "        self.size = size\n",
    "        self.memory = np.empty((size, 21), dtype=np.float32)\n",
    "        self.counter = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(self.size, self.counter)\n",
    "\n",
    "    def read(self, ind):\n",
    "        s = self.memory[ind, :9].astype(np.int32)\n",
    "        a = self.memory[ind, 9].astype(np.int32)\n",
    "        s_dash = self.memory[ind, 10:19].astype(np.int32)\n",
    "        r = self.memory[ind, 19]\n",
    "        e = self.memory[ind, 20]\n",
    "        return s, a, s_dash, r, e\n",
    "\n",
    "    def write(self, ind, s, a, s_dash, r, e):\n",
    "        self.memory[ind, :9] = s\n",
    "        self.memory[ind, 9] = a\n",
    "        self.memory[ind, 10:19] = s_dash\n",
    "        self.memory[ind, 19] = r\n",
    "        self.memory[ind, 20] = e\n",
    "\n",
    "    def append(self, s, a, s_dash, r, e):\n",
    "        ind = self.counter % self.size\n",
    "        self.write(ind, s, a, s_dash, r, e)\n",
    "        self.counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player（Q 学習）クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningPlayer(object):\n",
    "\n",
    "    def __init__(self, name, q_table, memory,\n",
    "                 reward_win=1., reward_draw=0., reward_lose=-1.,\n",
    "                 eps=0.05):\n",
    "        self.name = name\n",
    "        self.reward_win = reward_win\n",
    "        self.reward_draw = reward_draw\n",
    "        self.reward_lose = reward_lose\n",
    "        self.eps = eps\n",
    "\n",
    "        self.q_table = q_table\n",
    "        self.memory = memory\n",
    "\n",
    "        self.s_last = None\n",
    "        self.a_last = None\n",
    "\n",
    "        self.record = []\n",
    "\n",
    "    def action(self, board):\n",
    "        s = board\n",
    "\n",
    "        # decide the action\n",
    "        if np.random.random() < self.eps:\n",
    "            # epsilon-greedy. ランダムな行動\n",
    "            # ここを埋める（問２）\n",
    "        else:\n",
    "            # Q 値に従った行動\n",
    "            # ここを埋める（問２）\n",
    "\n",
    "        # memorise state and action\n",
    "        if self.s_last is not None:\n",
    "            self.memory.append(self.s_last, self.a_last, s, 0, 0)\n",
    "        self.s_last = board\n",
    "        self.a_last = a\n",
    "\n",
    "        return a\n",
    "\n",
    "    def finalize(self, condition, board):\n",
    "        s = board\n",
    "        if condition == WIN:\n",
    "            r = self.reward_win\n",
    "        elif condition == DRAW:\n",
    "            r = self.reward_draw\n",
    "        else:\n",
    "            r = self.reward_lose\n",
    "        self.memory.append(self.s_last, self.a_last, s, r, 1)\n",
    "\n",
    "        self.s_last = None\n",
    "        self.a_last = None\n",
    "\n",
    "        self.record.append(condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_table(q_table, memory, lr=0.1, gamma=0.95, n_epoch=1):\n",
    "    q_table_copy = copy.deepcopy(q_table)\n",
    "\n",
    "    for e in range(n_epoch):\n",
    "        for i in range(len(memory)):\n",
    "            s, a, s_dash, r, e = memory.read(i)\n",
    "            q_s_a = q_table.getitem(s, a)\n",
    "            if e == 0:\n",
    "                max_q_s_a_dash = np.max(q_table_copy.getitem(s_dash))\n",
    "            else:\n",
    "                max_q_s_a_dash = 0\n",
    "\n",
    "            # Q 値の更新\n",
    "            # ここを埋める（問２）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = RandomPlayer('乱太郎')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = QTable()\n",
    "memory = Memory()\n",
    "\n",
    "q_learning = QLearningPlayer('Q太郎', q_table, memory, eps=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i, end=', ')\n",
    "\n",
    "    if np.random.random() > 0.5:\n",
    "        X = random\n",
    "        O = q_learning\n",
    "    else:\n",
    "        X = q_learning\n",
    "        O = random\n",
    "\n",
    "    game = TicTacToe(X, O)\n",
    "    game.main_loop(print_game=False)\n",
    "    train_q_table(q_table, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToe(q_learning, random)\n",
    "game.main_loop(print_game=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勝率の変化\n",
    "wininig_Q = np.array(q_learning.record) == WIN\n",
    "\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot(np.cumsum(wininig_Q) / (np.arange(len(wininig_Q)) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問２では、Q 値を状態 $s$ を Key とする辞書で表現していた。  \n",
    "○×ゲームのように状態数に限りがある環境ではこれでも問題ないが、例えば TV ゲームのような複雑な環境では、状態数が爆発的に増加してしまい、すべての状態について Q 値を保持することが不可能になってしまう。\n",
    "\n",
    "この問題に対処する方法として、 **Q 値をニューラルネットワークで近似する** というものがある。  \n",
    "すなわち、状態 $s$ を入力とし、各行動 $a$ についての Q 値を出力するニューラルネットワークモデルを構築し、先の更新式に従ってモデルを訓練するのである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問3\n",
    "\n",
    "問２の QTable クラスの代わりに、ニューラルネットワークを用いた QFunction クラスを実装せよ。 \n",
    "また QFunction を行動決定に用いる NNQLearningPlayer を作成し、モデルを訓練せよ。  \n",
    "具体的には、\n",
    "$$\n",
    "loss = \\frac{1}{2}(Q^{new}_\\theta(s, a) - Q_\\theta(s, a))^2\n",
    "$$\n",
    "を最小化するように、QFunction のパラメタ $\\theta$ を更新してゆけば良い。\n",
    "\n",
    "#### 注意\n",
    "- ニューラルネットワークフレームワークを使用して良い\n",
    "- 本問の正解／不正解は選抜においてあまり重視しない（ただしインターンでは深層強化学習を用いたゲーム AI の作成に取り組んでいただく予定なので、挑戦してもらえると幸いです。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
